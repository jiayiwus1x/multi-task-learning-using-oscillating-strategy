{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A multi-layer perceptron for binary classification of MNIST handwritten digits.\"\"\"\n",
    "from __future__ import absolute_import, division\n",
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from autograd.misc.flatten import flatten\n",
    "from data import load_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as la\n",
    "\n",
    "\"\"\"Neural net functions. Simple fully connected neural network with sigmoid binary cross-entropy loss\"\"\"\n",
    "\n",
    "def init_random_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples,\n",
    "       one for each layer in the net.\"\"\"\n",
    "    return [(scale * rs.randn(m, n),   # weight matrix\n",
    "             scale * rs.randn(n))      # bias vector\n",
    "            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def neural_net_predict(params, inputs):\n",
    "    \"\"\"Implements a deep neural network for classification.\n",
    "       params is a list of (weights, bias) tuples.\n",
    "       inputs is an (N x D) matrix.\n",
    "       returns normalized class log-probabilities.\"\"\"\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = np.tanh(outputs)\n",
    "    return sigmoid(outputs)\n",
    "\n",
    "def cross_entropy(params, inputs, targets):\n",
    "    \"\"\"Cross entropy loss function. Measure difference \n",
    "       between probabilistic ouput and label\"\"\"\n",
    "    m = inputs.shape[0]\n",
    "    predict = neural_net_predict(params, inputs)\n",
    "    return (1/m)*np.sum(-targets*np.log(predict)-(1-targets)*np.log(1-predict))\n",
    "\n",
    "def accuracy(params, inputs, targets):\n",
    "    'Computes the accuracy of my neural network predictions'\n",
    "    predict_probability = neural_net_predict(params, inputs)\n",
    "    predict = predict_probability > 0.5\n",
    "    return np.mean(predict == targets)\n",
    "\n",
    "def batch_indices(iter):\n",
    "    'batches the data to use for stochastic optimization'\n",
    "    idx = iter % num_batches\n",
    "    return slice(idx * batch_size, (idx+1) * batch_size)\n",
    "\n",
    "# Define training objective\n",
    "def objective(params, iter, train_labels):\n",
    "    'evaluating the cross entropy loss on the batch. batch objective function.'\n",
    "    idx = batch_indices(iter)\n",
    "    return cross_entropy(params, train_images[idx], train_labels[idx])\n",
    "\n",
    "def record_accuracy(params, iter, train_labels, test_labels):\n",
    "    'output training error and testing error'\n",
    "    train_acc  = accuracy(params, train_images, train_labels)\n",
    "    test_acc  = accuracy(params, test_images, test_labels)\n",
    "    return train_acc, test_acc \n",
    "\n",
    "def adam(grad, x, train_lab, test_lab, callback=None, num_iters=100,\n",
    "         step_size=0.001, b1=0.9, b2=0.999, eps=10**-8):\n",
    "    \"\"\"Adam as described in http://arxiv.org/pdf/1412.6980.pdf.\n",
    "    It's basically RMSprop with momentum and some correction terms.\"\"\"\n",
    "    \n",
    "    x_flat,unflatten = flatten(x)\n",
    "    m = np.zeros(len(x_flat))\n",
    "    v = np.zeros(len(x_flat))\n",
    "    \n",
    "    train_perf = [0]\n",
    "    test_perf = [0]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        if test_perf[-1] > 0.98:\n",
    "            break\n",
    "        \n",
    "        g = flatten(grad(x, i, train_lab))[0]\n",
    "        x_flat = flatten(x)[0]\n",
    "        \n",
    "        if callback: \n",
    "            train_acc, test_acc = callback(x, i, train_lab, test_lab)\n",
    "            train_perf.append(train_acc)\n",
    "            test_perf.append(test_acc)\n",
    "            \n",
    "        m = (1 - b1) * g      + b1 * m  # First  moment estimate.\n",
    "        v = (1 - b2) * (g**2) + b2 * v  # Second moment estimate.\n",
    "        mhat = m / (1 - b1**(i + 1))    # Bias correction.\n",
    "        vhat = v / (1 - b2**(i + 1))\n",
    "        x = unflatten(x_flat - step_size*mhat/(np.sqrt(vhat) + eps))\n",
    "    return x, i, train_perf, test_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data and set experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading training data\n",
      "batch size per iteration is 256\n",
      "number of batches per epoch is 235\n",
      "number of epochs is 10\n",
      "max iterations per goal is 2350\n"
     ]
    }
   ],
   "source": [
    "N, train_images, train_labels_orig, test_images, test_labels_orig = load_mnist() # download and load data\n",
    "print(\"done loading training data\")\n",
    "\n",
    "# Define the layers of your neural network\n",
    "layer_sizes = [784,10,5,1]\n",
    "\n",
    "# Training parameters\n",
    "param_scale = 0.1 #scale of initial parameters(weight matrix and bias vector)\n",
    "batch_size = 256 \n",
    "num_epochs = 10\n",
    "step_size = 0.001\n",
    "num_batches = int(np.ceil(len(train_images) / batch_size))\n",
    "max_iter = num_epochs * num_batches\n",
    "\n",
    "print('batch size per iteration is', batch_size)\n",
    "print('number of batches per epoch is', num_batches)\n",
    "print('number of epochs is', num_epochs)\n",
    "print('max iterations per goal is', max_iter)\n",
    "\n",
    "# set goals\n",
    "goal1 = 4\n",
    "goal2 = 5\n",
    "\n",
    "# Get gradient of objective using autograd.\n",
    "objective_grad = grad(objective)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oscillation experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_osc_goal(train_labels_orig, test_labels_orig, goal1, goal2, init_params, obj_grad, step_size, max_it):\n",
    "\n",
    "    train_n = int(train_labels_orig.shape[0])\n",
    "    test_n = int(test_labels_orig.shape[0])\n",
    "\n",
    "    ### goal 1 ###\n",
    "    train_labels = train_labels_orig[:,goal1].reshape((train_n,1))\n",
    "    test_labels = test_labels_orig[:,goal1].reshape((test_n,1))\n",
    "\n",
    "    print('')\n",
    "    print('no oscillation experiment begin')\n",
    "    print('training goal 1')\n",
    "    opt_params_g1, iter_g1, tr_perf_g1, tst_perf_g1 = adam(obj_grad, init_params, train_labels, test_labels,\n",
    "                            step_size=step_size,num_iters=max_it, callback=record_accuracy)\n",
    "    \n",
    "    print('goal 1 converged')\n",
    "    print('training accuracy for goal 2 optimal =', tst_perf_g1[-1])\n",
    "    \n",
    "    ### goal 2 ###\n",
    "    train_labels = train_labels_orig[:,goal2].reshape((train_n,1))\n",
    "    test_labels = test_labels_orig[:,goal2].reshape((test_n,1))\n",
    "\n",
    "    print('training goal 2')\n",
    "    opt_params_g2, iter_g2, tr_perf_g2, tst_perf_g2 = adam(obj_grad, init_params, train_labels, test_labels,\n",
    "                            step_size=step_size,num_iters=max_it, callback=record_accuracy)\n",
    "    print('goal 2 converged')\n",
    "    print('training accuracy for goal 2 optimal =', tst_perf_g2[-1])\n",
    "    \n",
    "    l2_diff = la.norm(flatten(opt_params_g1)[0]-flatten(opt_params_g2)[0],2)\n",
    "    \n",
    "    return l2_diff,iter_g1,iter_g2\n",
    "\n",
    "def osc_goal(train_labels_orig, test_labels_orig, goal1, goal2, init_params, obj_grad, step_size, max_it, num_cycles):\n",
    "\n",
    "    train_n = int(train_labels_orig.shape[0])\n",
    "    test_n = int(test_labels_orig.shape[0])\n",
    "\n",
    "    train_labels_g1 = train_labels_orig[:,goal1].reshape((train_n,1))\n",
    "    test_labels_g1 = test_labels_orig[:,goal1].reshape((test_n,1))\n",
    "    \n",
    "    train_labels_g2 = train_labels_orig[:,goal2].reshape((train_n,1))\n",
    "    test_labels_g2 = test_labels_orig[:,goal2].reshape((test_n,1))\n",
    "    \n",
    "    print('')\n",
    "    print('oscillating goals experiment begin')\n",
    "    print('initial optimization')\n",
    "    print('training goal 1')\n",
    "    opt_params_g1, iter_g1, _, tst_perf_g1 = adam(obj_grad, init_params, train_labels_g1, test_labels_g1,\n",
    "                            step_size=step_size,num_iters=max_it, callback=record_accuracy)\n",
    "    \n",
    "    print('goal 1 converged')\n",
    "    print('training accuracy for goal 2 optimal =', tst_perf_g1[-1])\n",
    "    \n",
    "    cycle_counter = 0\n",
    "    \n",
    "    l2_list = []\n",
    "    iter_g1g2 = []\n",
    "    iter_g2g1 = []\n",
    "    \n",
    "    while cycle_counter < num_cycles:\n",
    "\n",
    "        print('')\n",
    "        print('cycle', cycle_counter+1)\n",
    "\n",
    "        print('training goal 2')\n",
    "        opt_params_g2, iter_g2, _, tst_perf_g2 = adam(obj_grad, opt_params_g1, train_labels_g2, test_labels_g2,\n",
    "                                step_size=step_size,num_iters=max_it, callback=record_accuracy)\n",
    "        print('goal 2 converged')\n",
    "        print('training accuracy for goal 2 optimal =', tst_perf_g2[-1])\n",
    "\n",
    "        \n",
    "        print('training goal 1')\n",
    "        opt_params_g1, iter_g1, _, tst_perf_g1 = adam(obj_grad, opt_params_g2, train_labels_g1, test_labels_g1,\n",
    "                                step_size=step_size,num_iters=max_it, callback=record_accuracy)\n",
    "\n",
    "        print('goal 1 converged')\n",
    "        print('training accuracy for goal 2 optimal =', tst_perf_g1[-1])\n",
    "        \n",
    "        iter_g1g2.append(iter_g2)\n",
    "        iter_g2g1.append(iter_g1)\n",
    "        l2_list.append(la.norm(flatten(opt_params_g1)[0]-flatten(opt_params_g2)[0],2))\n",
    "\n",
    "        cycle_counter += 1\n",
    "        \n",
    "    return l2_list,iter_g1g2,iter_g2g1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "no oscillation experiment begin\n",
      "training goal 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cseuser/anaconda3/lib/python3.6/site-packages/autograd/numpy/numpy_vjps.py:444: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return lambda g: g[idxs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goal 1 converged\n",
      "training accuracy for goal 2 optimal = 0.9805\n",
      "training goal 2\n",
      "goal 2 converged\n",
      "training accuracy for goal 2 optimal = 0.9801\n",
      "91.6492133140564\n"
     ]
    }
   ],
   "source": [
    "\n",
    "init_params = init_random_params(param_scale, layer_sizes)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "l2_no_osc, g1_iter, g2_iter = no_osc_goal(train_labels_orig, test_labels_orig, \n",
    "                                 goal1, goal2, init_params, objective_grad, step_size, max_iter)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g1_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oscillating goals experiment begin\n",
      "initial optimization\n",
      "training goal 1\n",
      "goal 1 converged\n",
      "training accuracy for goal 2 optimal = 0.9805\n",
      "\n",
      "cycle 1\n",
      "training goal 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1b5d816e2346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m l2_osc, g1g2_iter, g2g1_iter = osc_goal(train_labels_orig, test_labels_orig, \n\u001b[0;32m----> 8\u001b[0;31m                                         goal1, goal2, init_params, objective_grad, step_size, max_iter, num_cycles)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-572e02ffd24b>\u001b[0m in \u001b[0;36mosc_goal\u001b[0;34m(train_labels_orig, test_labels_orig, goal1, goal2, init_params, obj_grad, step_size, max_it, num_cycles)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training goal 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         opt_params_g2, iter_g2, _, tst_perf_g2 = adam(obj_grad, opt_params_g1, train_labels_g2, test_labels_g2,\n\u001b[0;32m---> 67\u001b[0;31m                                 step_size=step_size,num_iters=max_it, callback=record_accuracy)\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'goal 2 converged'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training accuracy for goal 2 optimal ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst_perf_g2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e3b21edb41b1>\u001b[0m in \u001b[0;36madam\u001b[0;34m(grad, x, train_lab, test_lab, callback, num_iters, step_size, b1, b2, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_lab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0mtrain_perf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtest_perf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e3b21edb41b1>\u001b[0m in \u001b[0;36mrecord_accuracy\u001b[0;34m(params, iter, train_labels, test_labels)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecord_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;34m'output training error and testing error'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mtest_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e3b21edb41b1>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(params, inputs, targets)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m'Computes the accuracy of my neural network predictions'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mpredict_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_net_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_probability\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e3b21edb41b1>\u001b[0m in \u001b[0;36mneural_net_predict\u001b[0;34m(params, inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m        returns normalized class log-probabilities.\"\"\"\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_cycles = 10\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "l2_osc, g1g2_iter, g2g1_iter = osc_goal(train_labels_orig, test_labels_orig, \n",
    "                                        goal1, goal2, init_params, objective_grad, step_size, max_iter, num_cycles)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g1g2_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f9d83eb2e632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg1g2_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'g1g2_iter' is not defined"
     ]
    }
   ],
   "source": [
    "print(l2_no_osc)\n",
    "print(l2_osc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[468, 119, 76, 63, 66, 96, 85, 81, 87, 77, 81, 96, 92, 95, 94, 98, 101, 110, 107, 101, 115, 133, 109, 118, 118, 112, 103, 108, 109, 111, 111, 108, 106, 103, 91, 85, 80, 80, 81, 81, 88, 89, 94, 98, 100, 105, 98, 113, 111, 100, 111, 106, 103, 106, 123, 131, 126, 126, 125, 125, 124, 122, 125, 133, 137, 134, 132, 133, 134, 134, 137, 136, 135, 140, 138, 136, 139, 139, 138, 143, 138, 135, 141, 143, 231, 145, 172, 137, 171, 159, 136, 133, 136, 137, 137, 140, 141, 138, 138, 139]\n",
      "[136, 95, 59, 69, 77, 65, 76, 73, 70, 65, 66, 72, 96, 71, 83, 80, 75, 77, 79, 82, 94, 91, 84, 86, 85, 84, 83, 86, 89, 91, 91, 91, 91, 89, 79, 75, 72, 72, 70, 72, 73, 74, 77, 80, 82, 86, 81, 93, 92, 85, 92, 87, 85, 87, 98, 103, 100, 97, 100, 100, 100, 98, 100, 103, 102, 103, 100, 100, 98, 100, 104, 97, 101, 109, 94, 115, 102, 99, 92, 111, 91, 110, 87, 111, 131, 114, 99, 96, 103, 102, 94, 95, 92, 86, 92, 93, 89, 90, 91, 93]\n"
     ]
    }
   ],
   "source": [
    "print(g1g2_iter)\n",
    "print(g2g1_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
